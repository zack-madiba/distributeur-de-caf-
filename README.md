# Analyse ComplÃ¨te des Ventes d'un Distributeur de CafÃ©

## I. Contexte du Projet

En tant que **Data Engineer** pour une enseigne spÃ©cialisÃ©e dans la vente de boissons chaudes Ã  New York, j'ai dÃ©veloppÃ© une solution complÃ¨te d'analyse des donnÃ©es de vente. L'entreprise exploite plusieurs magasins rÃ©partis sur diffÃ©rents quartiers et souhaite optimiser ses dÃ©cisions commerciales grÃ¢ce Ã  l'analyse de ses donnÃ©es transactionnelles.

### ProblÃ©matique Business
- Manque de visibilitÃ© sur les performances des diffÃ©rents points de vente
- Besoin d'identifier les tendances de consommation par pÃ©riode et localisation
- Optimisation de l'offre produit selon les prÃ©fÃ©rences clients
- AmÃ©lioration de la planification des stocks et des ressources

---

## II. Objectifs

### Objectifs Techniques
- [x] Collecter et ingÃ©rer les donnÃ©es depuis un fichier Excel
- [x] Concevoir et implÃ©menter une base de donnÃ©es PostgreSQL optimisÃ©e
- [x] DÃ©velopper un pipeline ETL robuste pour le nettoyage des donnÃ©es
- [x] CrÃ©er un modÃ¨le en Ã©toile (star schema) pour l'analyse
- [x] Enrichir les donnÃ©es avec la gÃ©olocalisation des magasins

### Objectifs Business
- [x] Analyser les performances de vente par magasin et pÃ©riode
- [x] Identifier les produits les plus populaires
- [x] Comprendre les patterns de consommation temporels
- [x] Fournir des insights exploitables aux Ã©quipes mÃ©tier

---

## III. Architecture Technique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fichier       â”‚    â”‚   Pipeline       â”‚    â”‚   Base de       â”‚
â”‚   Excel         â”‚â”€â”€â”€â–¶â”‚   ETL            â”‚â”€â”€â”€â–¶â”‚   DonnÃ©es       â”‚
â”‚   Source        â”‚    â”‚   Python         â”‚    â”‚   PostgreSQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                         â”‚
                              â–¼                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Enrichissement â”‚    â”‚   ModÃ©lisation  â”‚
                    â”‚   GÃ©olocalisationâ”‚    â”‚   Star Schema   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## IV. Pipeline de DonnÃ©es

### 1. Extraction des DonnÃ©es
- **Source** : Dataset Kaggle "Coffee Sales" (ahmedabbas757/coffee-sales)
- **Format** : Fichier Excel (.xlsx) tÃ©lÃ©chargÃ© automatiquement
- **Volume** : DonnÃ©es transactionnelles de mars 2024
- **Contenu** : Transactions avec horodatage, produits, prix, quantitÃ©s, types de paiement

### 2. Transformation et Nettoyage
```python
# TÃ©lÃ©chargement automatique depuis Kaggle
def download_dataset(path):
    path = kagglehub.dataset_download("ahmedabbas757/coffee-sales")
    files = os.listdir(path)
    excel_files = [f for f in files if f.endswith('.xlsx')]
    df = pd.read_excel(f"{path}/{excel_files[0]}")
    shutil.rmtree(path)  # Nettoyage automatique
    return df

# Enrichissement temporel
dfcopy["hour"] = pd.to_datetime(dfcopy["transaction_time"], format="%H:%M:%S").dt.hour
dfcopy["day"] = dfcopy["transaction_date"].dt.day
dfcopy["month"] = dfcopy["transaction_date"].dt.strftime("%B")
dfcopy["weekday"] = dfcopy["transaction_date"].dt.strftime("%A")
dfcopy["time_slot"] = dfcopy["hour"].apply(get_tranche_horaire)
dfcopy["season"] = dfcopy["month"].map(seasons)

# Calcul du chiffre d'affaires
dfcopy["recipe"] = dfcopy["transaction_qty"] * dfcopy["unit_price"]
```

### 3. Enrichissement GÃ©ographique
- Utilisation de l'API Nominatim pour gÃ©olocaliser les magasins
- Ajout des coordonnÃ©es latitude/longitude
- Gestion des erreurs et timeout pour la robustesse

```python
geolocator = Nominatim(user_agent="geoapi")
for store in unique_stores:
    location = geolocator.geocode(store + ", New York, USA", timeout=10)
    # Traitement et sauvegarde des coordonnÃ©es
```

### 4. Chargement en Base
- Insertion en lots (chunks) de 1700 enregistrements
- Gestion des conflits avec `ON CONFLICT DO NOTHING`
- Optimisation avec indexation multi-colonnes

---

## V. ModÃ©lisation des DonnÃ©es

### SchÃ©ma en Ã‰toile (Star Schema)

#### Table de Faits
**`fact_sales`** - Table centrale des transactions
- `id_transaction` (PK)
- `product_id` (FK)
- `location_id` (FK)
- `date_id` (FK)
- `season_id` (FK)
- `quantity`
- `unit_price`

#### Tables de Dimensions

**`product`** - Catalogue produit
- `product_id` (PK)
- `product_detail`
- `id_categorie` (FK)
- `id_type` (FK)
- `unit_price`

**`location_sales`** - Points de vente
- `id_location` (PK)
- `store_location`
- `latitude`
- `longitude`

**`date_sales`** - Dimension temporelle
- `id_date` (PK)
- `full_date`
- `day`, `weekday`, `month`, `year`
- `hour`, `time_slot`

**`product_categorie`** & **`product_type`** - Classifications produit

**`season`** - Saisons

### Optimisations Performantes
```sql
-- Index composites pour les requÃªtes analytiques
CREATE INDEX idx_fact_composite ON fact_sales(product_id, date_id);
CREATE INDEX idx_fact_sales_location ON fact_sales(location_id);
-- Analyse automatique des statistiques
ANALYZE fact_sales;
```

---

## VI. Traitement et Enrichissement

### FonctionnalitÃ©s DÃ©veloppÃ©es

#### 1. TÃ©lÃ©chargement AutomatisÃ© depuis Kaggle
- **API Kaggle** pour rÃ©cupÃ©ration automatique des donnÃ©es
- **DÃ©tection automatique** des fichiers Excel dans le dataset
- **Nettoyage automatique** des fichiers temporaires aprÃ¨s traitement

```python
def download_dataset(path):
    path = kagglehub.dataset_download("ahmedabbas757/coffee-sales")
    files = os.listdir(path)
    excel_files = [f for f in files if f.endswith('.xlsx')]
    if excel_files:
        df = pd.read_excel(f"{path}/{excel_files[0]}")
        shutil.rmtree(path)  # Nettoyage des fichiers temporaires
    return df
```
- **Tranches horaires** : matin, midi, aprÃ¨s-midi, soir, nuit
- **Dimensions temporelles** : jour, semaine, mois, saison
- **Analyse des patterns** de consommation

#### 2. Segmentation Temporelle
- **Tranches horaires** : matin, midi, aprÃ¨s-midi, soir, nuit
- **Dimensions temporelles** : jour, semaine, mois, saison
- **Analyse des patterns** de consommation

#### 3. GÃ©olocalisation Automatique
- **API Nominatim** pour la gÃ©olocalisation des adresses
- **CoordonnÃ©es GPS** pour chaque point de vente
- **Gestion d'erreurs** robuste avec retry logic

#### 4. Calculs MÃ©tier
- **Chiffre d'affaires** par transaction
- **MÃ©triques agrÃ©gÃ©es** par pÃ©riode et localisation
- **KPIs business** ready-to-use

---

## VII. RÃ©sultats et Insights

### MÃ©triques ClÃ©s CalculÃ©es
- Chiffre d'affaires par magasin et pÃ©riode
- Top produits par catÃ©gorie et type
- Distribution des ventes par tranche horaire
- Performance saisonniÃ¨re des points de vente
- Analyse gÃ©ographique des performances

### Structure de DonnÃ©es OptimisÃ©e
- **149,116 transactions** traitÃ©es et nettoyÃ©es
- **ModÃ¨le normalisÃ©** en 3NF avec star schema
- **Performance optimisÃ©e** avec indexation stratÃ©gique
- **DonnÃ©es gÃ©olocalisÃ©es** prÃªtes pour la cartographie

### RequÃªtes d'Exploitation DÃ©veloppÃ©es

#### 1. Top 5 Produits par Recette Globale
```sql
-- Analyse globale des meilleures performances produit
SELECT 
    p.product_detail AS "Produit", 
    pc.categorie AS Categorie,
    pt.type AS Type,
    TO_CHAR(SUM(fs.quantity * fs.unit_price), 'FM999999990.00 â‚¬') AS Recette,
    ls.store_location AS Ville
FROM fact_sales AS fs
JOIN product AS p ON fs.product_id = p.product_id
JOIN product_categorie AS pc ON pc.id_categorie = p.id_categorie  
JOIN product_type AS pt ON pt.id_type = p.id_type
JOIN location_sales AS ls ON ls.id_location = fs.location_id
JOIN date_sales AS ds ON ds.id_date = fs.date_id
GROUP BY p.product_detail, pc.categorie, pt.type, ls.store_location
ORDER BY SUM(fs.quantity * fs.unit_price) DESC
LIMIT 5;
```

#### 2. Top 5 Produits des Deux Derniers Mois
```sql
-- Analyse de performance rÃ©cente avec CTE pour gestion des dates
WITH limites_dates AS (
    SELECT 
        MAX(full_date) AS max_date,
        DATE_TRUNC('month', MAX(full_date)) - INTERVAL '1 month' AS debut_avant_dernier_mois
    FROM date_sales
)
SELECT 
    p.product_detail AS "Produit", 
    pc.categorie AS Categorie,
    pt.type AS Type,
    SUM(fs.quantity * fs.unit_price) AS Recette,
    ls.store_location AS Ville
FROM fact_sales fs
JOIN product p ON fs.product_id = p.product_id
JOIN product_categorie pc ON pc.id_categorie = p.id_categorie
JOIN product_type pt ON pt.id_type = p.id_type
JOIN location_sales ls ON ls.id_location = fs.location_id
JOIN date_sales ds ON ds.id_date = fs.date_id
JOIN limites_dates ld ON TRUE
WHERE ds.full_date >= ld.debut_avant_dernier_mois
  AND ds.full_date <= ld.max_date
GROUP BY p.product_detail, pc.categorie, pt.type, ls.store_location
ORDER BY Recette DESC
LIMIT 5;
```

#### 3. Meilleur Produit par Recette Mensuelle
```sql
-- Ã‰volution du leadership produit avec fonctions de fenÃªtrage
WITH ventes_mensuelles AS (
    SELECT 
        p.product_detail AS produit,
        DATE_TRUNC('month', ds.full_date) AS mois,
        SUM(fs.quantity * fs.unit_price) AS recette
    FROM fact_sales fs
    JOIN product p ON fs.product_id = p.product_id
    JOIN date_sales ds ON fs.date_id = ds.id_date
    GROUP BY produit, mois
),
classement AS (
    SELECT *,
           RANK() OVER (PARTITION BY mois ORDER BY recette DESC) AS rang
    FROM ventes_mensuelles
)
SELECT mois, produit, recette
FROM classement
WHERE rang = 1
ORDER BY mois;
```

#### 4. Produit le Plus ConsommÃ© par Mois (QuantitÃ©)
```sql
-- Analyse de volume avec ROW_NUMBER pour Ã©liminer les ex-aequo
WITH ventes_mensuelles AS (
    SELECT 
        ds.id_date,
        p.product_detail as "Nom du produit",
        DATE_TRUNC('month', ds.full_date) AS mois,
        SUM(fs.quantity) AS quantite
    FROM fact_sales fs
    JOIN date_sales ds ON fs.date_id = ds.id_date
    JOIN product p ON fs.product_id = p.product_id
    GROUP BY mois, "Nom du produit", ds.id_date
),
classement AS (
    SELECT *,
           ROW_NUMBER() OVER (PARTITION BY mois ORDER BY quantite DESC) AS rang
    FROM ventes_mensuelles
)
SELECT mois, "Nom du produit", quantite
FROM classement
WHERE rang = 1
ORDER BY mois;
```

### Explications Techniques des RequÃªtes

**CTE (Common Table Expressions)** : Utilisation pour structurer les requÃªtes complexes et amÃ©liorer la lisibilitÃ©, notamment pour les calculs de dates dynamiques.

**Fonctions de FenÃªtrage** : 
- `RANK()` : Permet les ex-aequo avec des rangs identiques
- `ROW_NUMBER()` : Attribution de rangs uniques mÃªme en cas d'Ã©galitÃ©

**Formatage des DonnÃ©es** : 
- `TO_CHAR()` pour le formatage monÃ©taire avec devise
- `DATE_TRUNC()` pour l'agrÃ©gation temporelle par mois

**Jointures OptimisÃ©es** : Star schema permettant des jointures efficaces entre la table de faits et les dimensions.

---

## Notes Techniques

### Stack Technique
- **Python 3.x** - Langage principal
- **PostgreSQL** - Base de donnÃ©es relationnelle
- **SQLAlchemy** - ORM et gestion des connexions
- **Pandas** - Manipulation et analyse des donnÃ©es
- **GeoPy** - GÃ©olocalisation automatique
- **psycopg2** - Connecteur PostgreSQL
- **KaggleHub** - TÃ©lÃ©chargement automatique des datasets

### Librairies Python
```python
import psycopg2
import pandas as pd
from sqlalchemy import create_engine, text
from geopy.geocoders import Nominatim
import kagglehub
import time
import locale
import os
import shutil
```

---

## IX. Guide d'Installation

### PrÃ©requis
- Python 3.8+
- PostgreSQL 12+
- Compte Kaggle configurÃ© avec API
- Packages Python requis (voir requirements.txt)

### Installation
```bash
# Cloner le repository
git clone [repository-url]
cd coffee-sales-analysis

# Installer les dÃ©pendances
pip install -r requirements.txt
pip install kagglehub

# Configurer l'API Kaggle (authentification requise)
# Placer le fichier kaggle.json dans ~/.kaggle/

# Configurer PostgreSQL
# CrÃ©er un utilisateur et ajuster les credentials dans le script
```

### Configuration Base de DonnÃ©es
```python
# ParamÃ¨tres de connexion Ã  ajuster
conn = psycopg2.connect(
    host="localhost",
    dbname="postgres", 
    user="postgres",
    password="1212",
    port="5432"
)
```

---

## X. Utilisation

### ExÃ©cution du Pipeline Complet
```bash
python coffee_sales_etl.py
```

### Ã‰tapes d'ExÃ©cution
1. **TÃ©lÃ©chargement** automatique depuis Kaggle via API
2. **CrÃ©ation** de la base de donnÃ©es `coffee_sales`
3. **Nettoyage** et enrichissement des donnÃ©es
4. **GÃ©olocalisation** automatique des magasins
5. **CrÃ©ation** du modÃ¨le en Ã©toile
6. **Indexation** et optimisation des performances

### RÃ©sultat Attendu
```
âœ… Dataset tÃ©lÃ©chargÃ© depuis Kaggle
âœ… Base 'coffee_sales' crÃ©Ã©e
âœ… Dataframe nettoyÃ©: 149,116 lignes & 18 colonnes
âœ… Tables dimensionnelles crÃ©Ã©es
âœ… DonnÃ©es chargÃ©es et indexÃ©es
âœ… Pipeline ETL terminÃ© avec succÃ¨s
```

---

## ðŸ”® AmÃ©liorations Futures

### Ã‰volutions Techniques
- [ ] **Automatisation** avec Apache Airflow
- [ ] **Containerisation** avec Docker
- [ ] **Tests unitaires** et intÃ©gration continue
- [ ] **Monitoring** et alerting des pipelines
- [ ] **API REST** pour l'accÃ¨s aux donnÃ©es

### Ã‰volutions Fonctionnelles
- [ ] **Dashboard interactif** avec Streamlit/Dash
- [ ] **PrÃ©dictions de vente** avec ML
- [ ] **Analyse de sentiment** des reviews clients
- [ ] **Optimisation des stocks** avec algorithmes
- [ ] **Segmentation client** avancÃ©e

### Ã‰volutions Data
- [ ] **Data Lake** pour historique long terme
- [ ] **Streaming** pour donnÃ©es temps rÃ©el
- [ ] **Data Quality** avec Great Expectations
- [ ] **Catalogue de donnÃ©es** avec DataHub

---

## ðŸ“ Notes Techniques

### Bonnes Pratiques ImplÃ©mentÃ©es
- **Gestion des erreurs** robuste
- **Transactions atomiques** en base
- **Indexation optimisÃ©e** pour les performances
- **Code modulaire** et rÃ©utilisable
- **Documentation** inline du code

### Points d'Attention
- **Pause entre requÃªtes** API Nominatim (1 sec) pour respecter les conditions d'usage
- **Timeout** de gÃ©olocalisation configurÃ© Ã  10s
- **Gestion mÃ©moire** avec chunks de 1700 lignes
- **Validation des types** de donnÃ©es

---

## Contributeurs
- **[Votre Nom]** - Data Engineer Principal

## License
Ce projet est sous licence MIT - voir le fichier [LICENSE.md](LICENSE.md) pour plus de dÃ©tails.

---
*Projet rÃ©alisÃ© dans le cadre de l'optimisation des performances commerciales d'une chaÃ®ne de cafÃ©s new-yorkaise*
