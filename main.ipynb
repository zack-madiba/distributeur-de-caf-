{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b160ebb0",
   "metadata": {},
   "source": [
    "# Debut du Projet : KaggleHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40188474",
   "metadata": {},
   "source": [
    "# I - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93165b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ahmedabbas757/coffee-sales?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.23M/8.23M [00:00<00:00, 9.54MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ahmedabbas757/coffee-sales?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.23M/8.23M [00:00<00:00, 9.56MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>transaction_qty</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>product_id</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>07:06:11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Gourmet brewed coffee</td>\n",
       "      <td>Ethiopia Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>07:08:56</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>57</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>07:14:04</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>59</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Drinking Chocolate</td>\n",
       "      <td>Hot chocolate</td>\n",
       "      <td>Dark chocolate Lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>07:20:24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Drip coffee</td>\n",
       "      <td>Our Old Time Diner Blend Sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>07:22:41</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>57</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Tea</td>\n",
       "      <td>Brewed Chai tea</td>\n",
       "      <td>Spicy Eye Opener Chai Lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id transaction_date transaction_time  transaction_qty  \\\n",
       "0               1       2023-01-01         07:06:11                2   \n",
       "1               2       2023-01-01         07:08:56                2   \n",
       "2               3       2023-01-01         07:14:04                2   \n",
       "3               4       2023-01-01         07:20:24                1   \n",
       "4               5       2023-01-01         07:22:41                2   \n",
       "\n",
       "   store_id   store_location  product_id  unit_price    product_category  \\\n",
       "0         5  Lower Manhattan          32         3.0              Coffee   \n",
       "1         5  Lower Manhattan          57         3.1                 Tea   \n",
       "2         5  Lower Manhattan          59         4.5  Drinking Chocolate   \n",
       "3         5  Lower Manhattan          22         2.0              Coffee   \n",
       "4         5  Lower Manhattan          57         3.1                 Tea   \n",
       "\n",
       "            product_type               product_detail  \n",
       "0  Gourmet brewed coffee                  Ethiopia Rg  \n",
       "1        Brewed Chai tea     Spicy Eye Opener Chai Lg  \n",
       "2          Hot chocolate            Dark chocolate Lg  \n",
       "3            Drip coffee  Our Old Time Diner Blend Sm  \n",
       "4        Brewed Chai tea     Spicy Eye Opener Chai Lg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Téléchargement et affichage du jeu de données \"Coffee Sales\" depuis Kaggle\n",
    "# Assurez-vous d'avoir installé kagglehub avec `pip install kagglehub`\n",
    "def download_dataset(path):\n",
    "    df = None\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(\"ahmedabbas757/coffee-sales\")\n",
    "        \n",
    "        files = os.listdir(path)\n",
    "        excel_files = [f for f in files if f.endswith('.xlsx')]\n",
    "        \n",
    "        if excel_files:\n",
    "            df = pd.read_excel(f\"{path}/{excel_files[0]}\")\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            print(\"Aucun fichier CSV trouvé dans le dossier téléchargé.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "download_dataset(\"coffee_sales\")\n",
    "\n",
    "\n",
    "df = download_dataset(\"coffee_sales\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbf62a",
   "metadata": {},
   "source": [
    "# I - execo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "df = pd.read_excel(\"coffe_sales_.xlsx\")\n",
    "print('shape :',df.shape)\n",
    "colonnes = df.columns.to_list()\n",
    "print(f\"{len(colonnes)}colonnes : {colonnes}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3d792b",
   "metadata": {},
   "source": [
    "# II - Creation de la BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "df = pd.read_excel(\"coffe_sales_.xlsx\")\n",
    "print('donné du csv brut :',df.shape)\n",
    "colonnes = df.columns.to_list()\n",
    "print(f\"{len(colonnes)}colonnes : {colonnes}\")\n",
    "df.head()\n",
    "\n",
    "# Connexion à la base postgres (par défaut pour créer une autre base)\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    port=\"5432\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Vérifie si la BDD existe déjà\n",
    "cur.execute(\"SELECT 1 FROM pg_database WHERE datname = 'coffee_sales'\")\n",
    "exists = cur.fetchone()\n",
    "\n",
    "if not exists:\n",
    "    cur.execute(\"CREATE DATABASE coffee_sales ENCODING 'LATIN1';\")\n",
    "    print(\"Base 'coffee_sales' créée.\")\n",
    "else:\n",
    "    print(\"La base 'coffee_sales' existe déjà.\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Connexion à la base coffee_sales\n",
    "comm = psycopg2.connect(\n",
    "    dbname=\"coffee_sales\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "cur = comm.cursor()\n",
    "cur.execute(\"SELECT 1\")\n",
    "if cur.fetchone():\n",
    "    print(\"Connexion à la base 'coffee_sales' réussie.\")\n",
    "cur.close()\n",
    "comm.close()\n",
    "\n",
    "# Création de la table à partir du DataFrame\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "## Envoi du DataFrame dans la BDD\n",
    "df.to_sql(\n",
    "    'coffee_sales', \n",
    "    engine, if_exists='replace',\n",
    "    chunksize=1700, # insertion par bloc de 1700 lignes pour éviter les erreurs de mémoire\n",
    "    index=False\n",
    ")\n",
    "print(\"Table 'coffee_sales' créée avec succès.\")\n",
    "\n",
    "\n",
    "query = '''select * from coffee_sales;'''\n",
    "tables_check = \"\"\" SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\"SELECT * FROM coffee_sales\", engine)\n",
    "tables_check =pd.read_sql(tables_check, engine)\n",
    "display(tables_check)\n",
    "dfcopy = df.copy()\n",
    "dfcopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ef864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d2db980",
   "metadata": {},
   "source": [
    "# III - Cleaned data et enrichissement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import time\n",
    "locale.setlocale(locale.LC_TIME, 'en_EN.UTF-8')\n",
    "from geopy.geocoders import Nominatim\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"coffe_sales_.xlsx\")\n",
    "print('nombre de lignes du csv brute :',df.shape)\n",
    "colonnes = df.columns.to_list()\n",
    "print(f\"{len(colonnes)}colonnes : {colonnes}\")\n",
    "df.head()\n",
    "\n",
    "# Connexion à la base postgres (par défaut pour créer une autre base)\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    port=\"5432\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Vérifie si la BDD existe déjà\n",
    "cur.execute(\"SELECT 1 FROM pg_database WHERE datname = 'coffee_sales'\")\n",
    "exists = cur.fetchone()\n",
    "\n",
    "if not exists:\n",
    "    cur.execute(\"CREATE DATABASE coffee_sales ENCODING 'LATIN1';\")\n",
    "    print(\"Base 'coffee_sales' créée.\")\n",
    "else:\n",
    "    print(\"La base 'coffee_sales' existe déjà.\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# instance de Connexion à la base coffee_sales\n",
    "comm = psycopg2.connect(\n",
    "    dbname=\"coffee_sales\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "cur = comm.cursor()\n",
    "cur.execute(\"SELECT 1\")\n",
    "if cur.fetchone():\n",
    "    print(\"Connexion à la base 'coffee_sales' réussie.\")\n",
    "cur.close()\n",
    "comm.close()\n",
    "\n",
    "# Création de la table à partir du DataFrame\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "## Envoi du DataFrame dans la BDD\n",
    "df.to_sql(\n",
    "    'coffee_sales', \n",
    "    engine, if_exists='replace',\n",
    "    chunksize=1700, # insertion par bloc de 1700 lignes pour éviter les erreurs de mémoire\n",
    "    index=False\n",
    ")\n",
    "print(\"Table 'coffee_sales' créée avec succès.\")\n",
    "\n",
    "\n",
    "query = '''select * from coffee_sales;'''\n",
    "tables_check = \"\"\" SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\"SELECT * FROM coffee_sales\", engine)\n",
    "tables_check =pd.read_sql(tables_check, engine)\n",
    "display(tables_check)\n",
    "dfcopy = df.copy() # on cré une copie de notre df pour avoir un df 'backup' des données brutes\n",
    "\n",
    "# oncré une colonne \"recipe\" qui nous aidera à calculer la recettes\n",
    "\n",
    "dfcopy[\"recipe\"]= dfcopy[\"transaction_qty\"] * dfcopy[\"unit_price\"] \n",
    "\n",
    "# convertissons la colonne transaction_date en datetime\n",
    "\n",
    "dfcopy[\"transaction_date\"] = pd.to_datetime(dfcopy[\"transaction_date\"])\n",
    "#dfcopy[\"transaction_time\"] = pd.to_datetime(dfcopy[\"transaction_time\"])\n",
    "\n",
    "# on cré les colonne temporelles pour affiné nos métrique\n",
    "\n",
    "dfcopy[\"hour\"] = pd.to_datetime(dfcopy[\"transaction_time\"], format=\"%H:%M:%S\").dt.hour# heure de la journée\n",
    "dfcopy[\"day\"] = dfcopy[\"transaction_date\"].dt.day # jour du mois\n",
    "dfcopy[\"month\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%B\") # mois de l'année\n",
    "dfcopy[\"weekday\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%A\") # jour de la semaine\n",
    "dfcopy[\"year\"]=dfcopy[\"transaction_date\"].dt.year \n",
    "\n",
    "# on cree une colonne  avec la date complète pour faciliter l'analyse et on supprimme l'ancienne colonne transaction_date\n",
    "\n",
    "dfcopy['full_date'] = dfcopy['transaction_date'].dt.strftime('%A, %#d %B %Y') +' '+ dfcopy['transaction_time'].astype(str)\n",
    "time.sleep(1)\n",
    "\n",
    "# on cré une colonne de la tranche horaire\n",
    "def get_tranche_horaire(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"matin\"\n",
    "    elif 12 <= hour < 14:\n",
    "        return \"midi\"\n",
    "    elif 14 <= hour < 18:\n",
    "        return \"après-midi\"\n",
    "    elif 18 <= hour < 22:\n",
    "        return \"soir\"\n",
    "    else:\n",
    "        return \"nuit\"\n",
    "\n",
    "dfcopy[\"time_slot\"] = dfcopy[\"hour\"].apply(get_tranche_horaire)\n",
    "\n",
    "# cré une colonne pour la saison pour une métrique plus poussée\n",
    "\n",
    "spring   = [\"March\", \"April\", \"May\"] #pritemps\n",
    "summer   = [\"June\", \"July\", \"August\"] # été\n",
    "autumn   = [\"September\", \"October\", \"November\"] # automne\n",
    "winter   = [\"December\", \"January\", \"February\"] #hiver\n",
    "\n",
    "dfcopy[\"season\"]= \"\"\n",
    "\n",
    "def get_saison(month):\n",
    "    if month in spring:\n",
    "        return \"spring\"\n",
    "    if month in summer:\n",
    "        return \"summer\"\n",
    "    if month in autumn:\n",
    "        return autumn\n",
    "    else:\n",
    "        return \"winter\"\n",
    "    \n",
    "dfcopy[\"season\"]=dfcopy[\"month\"].apply(get_saison)\n",
    "\n",
    "# on reorganise nos lignes par \"id_transaction\" ordre croissant\n",
    "\n",
    "dfcopy = dfcopy.sort_values(by=\"product_id\", ascending=True)\n",
    "\n",
    "# On cré deux colonnes 'Longitude' et 'lattitude' et on enrichit avec des données externes, pour une carto graphie\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Exemple : liste unique des stores depuis ton dataframe\n",
    "unique_stores = dfcopy['store_location'].dropna().unique()\n",
    "\n",
    "# Initialise Nominatim\n",
    "geolocator = Nominatim(user_agent=\"metabase_mapping\")\n",
    "\n",
    "# Stockage des résultats\n",
    "locations = []\n",
    "\n",
    "for store in unique_stores:\n",
    "    try:\n",
    "        # Utilise un nom de ville + \"New York\" ou autre si nécessaire\n",
    "        location = geolocator.geocode(store + \", New York, USA\", timeout=10)\n",
    "        if location:\n",
    "            print(f\"{store} => {location.latitude}, {location.longitude}\")\n",
    "            locations.append({\n",
    "                \"store_name\": store,\n",
    "                \"latitude\": location.latitude,\n",
    "                \"longitude\": location.longitude\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Pas trouvé : {store}\")\n",
    "            locations.append({\n",
    "                \"store_name\": store,\n",
    "                \"latitude\": None,\n",
    "                \"longitude\": None\n",
    "            })\n",
    "        time.sleep(1)  # Pour éviter de se faire bloquer par l'API\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {store}: {e}\")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "store_geo_df = pd.DataFrame(locations)\n",
    "\n",
    "# Visualisation ou export\n",
    "print(store_geo_df.head())\n",
    "store_geo_df.to_csv(\"store_locations_with_geo.csv\", index=False)\n",
    "\n",
    "coord_dict = {\n",
    "    \"Lower Manhattan\": [\"40.7135482\", \"-74.0054261\"],\n",
    "    \"Hell's Kitchen\": [\"40.7644228\", \"-73.9923918\"],\n",
    "    \"Astoria\": [\"40.7720145\", \"-73.9302673\"]\n",
    "}\n",
    "\n",
    "dfcopy[[\"latitude\", \"longitude\"]] = pd.DataFrame(dfcopy[\"store_location\"].map(coord_dict).tolist(), index=dfcopy.index)\n",
    "# Conversion des colonnes latitude et longitude en float (avec gestion des erreurs)\n",
    "dfcopy['latitude'] = pd.to_numeric(dfcopy['latitude'], errors='coerce')\n",
    "dfcopy['longitude'] = pd.to_numeric(dfcopy['longitude'], errors='coerce') #convertir la longitude et la latitude en float pour une visu map\n",
    "dfcopy['recipe'] = pd.to_numeric(dfcopy['recipe'], errors='coerce')\n",
    "dfcopy = dfcopy.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "dfcopy.to_sql(\n",
    "    'coffee_sales', \n",
    "    engine, if_exists='replace',\n",
    "    chunksize=1700, # insertion par bloc de 1700 lignes pour éviter les erreurs de mémoire\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Reorganisons notre dataframe \n",
    "\n",
    "colonnes = ['product_id','product_detail','product_category','product_type',\n",
    "            'unit_price','transaction_id','transaction_qty', 'transaction_date', \n",
    "            'transaction_time','store_id', 'store_location', 'recipe', \n",
    "            'weekday', 'day', 'month', 'hour', 'time_slot', 'season','year','full_date', 'latitude','longitude']\n",
    "dfcopy = dfcopy[colonnes]\n",
    "print('nombre de lignes du csv nettoyé :',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882984bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy1 = dfcopy\n",
    "dfcopy2 = dfcopy1.dropna(subset=['latitude', 'longitude'])\n",
    "dfcopy2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bed84",
   "metadata": {},
   "source": [
    "# Requetes SQL de creation des tables  et insertion des donnée depuis la table coffee_sales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Configuration de la connexion à la base de données\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "# Requêtes de création des tables (dimensions -> faits)\n",
    "table_product_categorie = text(\"\"\"CREATE TABLE IF NOT EXISTS product_categorie (\n",
    "  id_categorie SERIAL PRIMARY KEY,\n",
    "  categorie VARCHAR(50) UNIQUE\n",
    ");\"\"\")\n",
    "\n",
    "table_product_type = text(\"\"\"CREATE TABLE IF NOT EXISTS product_type (\n",
    "  id_type SERIAL PRIMARY KEY,\n",
    "  type VARCHAR(50) UNIQUE\n",
    ");\"\"\")\n",
    "\n",
    "table_location_sales = text(\"\"\"CREATE TABLE IF NOT EXISTS location_sales (\n",
    "  id_location SERIAL PRIMARY KEY,\n",
    "  store VARCHAR(100) UNIQUE,\n",
    "  latitude FLOAT,\n",
    "  longitude FLOAT\n",
    ");\"\"\")\n",
    "\n",
    "table_date_sales = text(\"\"\"CREATE TABLE IF NOT EXISTS date_sales (\n",
    "  id_date SERIAL PRIMARY KEY,\n",
    "  full_date DATE UNIQUE,\n",
    "  day INT,\n",
    "  weekday VARCHAR(50),\n",
    "  month VARCHAR(50),\n",
    "  year INT,\n",
    "  hour INT,\n",
    "  time_slot VARCHAR(25)\n",
    ");\"\"\")\n",
    "\n",
    "table_season = text(\"\"\"CREATE TABLE IF NOT EXISTS season (\n",
    "  id_season SERIAL PRIMARY KEY,\n",
    "  name VARCHAR(50) UNIQUE\n",
    ");\"\"\")\n",
    "\n",
    "table_product = text(\"\"\"CREATE TABLE IF NOT EXISTS product (\n",
    "  product_id SERIAL PRIMARY KEY,\n",
    "  product_detail VARCHAR(255) UNIQUE,\n",
    "  id_categorie INT REFERENCES product_categorie(id_categorie),\n",
    "  id_type INT REFERENCES product_type(id_type),\n",
    "  unit_price DECIMAL(10,2)\n",
    ");\"\"\")\n",
    "\n",
    "table_fact_sales = text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_sales (\n",
    "  id_transaction INT PRIMARY KEY,  -- on récupère l'ID existant de coffee_sales\n",
    "  product_id INT REFERENCES product(product_id),\n",
    "  location_id INT REFERENCES location_sales(id_location),\n",
    "  date_id INT REFERENCES date_sales(id_date),\n",
    "  season_id INT REFERENCES season(id_season),\n",
    "  quantity INT,\n",
    "  unit_price DECIMAL(10,2)\n",
    ");\"\"\")\n",
    "\n",
    "\n",
    "# Requêtes d'insertion des données\n",
    "insert_product_categorie = text(\"\"\"\n",
    "INSERT INTO product_categorie (categorie)\n",
    "SELECT DISTINCT product_category FROM coffee_sales\n",
    "ON CONFLICT (categorie) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_product_type = text(\"\"\"\n",
    "INSERT INTO product_type (type)\n",
    "SELECT DISTINCT product_type FROM coffee_sales\n",
    "ON CONFLICT (type) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_location_sales = text(\"\"\"\n",
    "INSERT INTO location_sales (store, latitude, longitude)\n",
    "SELECT DISTINCT store_id, latitude, longitude FROM coffee_sales\n",
    "ON CONFLICT (store) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_date_sales = text(\"\"\"\n",
    "INSERT INTO date_sales (full_date, day, weekday, month, year, hour, time_slot)\n",
    "SELECT DISTINCT \n",
    "    transaction_date::DATE,\n",
    "    day,\n",
    "    weekday,\n",
    "    month,\n",
    "    year,\n",
    "    hour,\n",
    "    time_slot\n",
    "FROM coffee_sales\n",
    "ON CONFLICT (full_date) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_season = text(\"\"\"\n",
    "INSERT INTO season (name)\n",
    "SELECT DISTINCT season FROM coffee_sales\n",
    "ON CONFLICT (name) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_product = text(\"\"\"\n",
    "INSERT INTO product (product_detail, id_categorie, id_type, unit_price)\n",
    "SELECT DISTINCT \n",
    "    cs.product_detail,\n",
    "    pc.id_categorie,\n",
    "    pt.id_type,\n",
    "    cs.unit_price\n",
    "FROM coffee_sales cs\n",
    "JOIN product_categorie pc ON cs.product_category = pc.categorie\n",
    "JOIN product_type pt ON cs.product_type = pt.type\n",
    "ON CONFLICT (product_detail) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_fact_sales = text(\"\"\"\n",
    "INSERT INTO fact_sales (\n",
    "    id_transaction,\n",
    "    product_id,\n",
    "    location_id,\n",
    "    date_id,\n",
    "    season_id,\n",
    "    quantity,\n",
    "    unit_price\n",
    ")\n",
    "SELECT \n",
    "    cs.transaction_id,\n",
    "    p.product_id,\n",
    "    ls.id_location,\n",
    "    ds.id_date,\n",
    "    s.id_season,\n",
    "    cs.transaction_qty,\n",
    "    cs.unit_price\n",
    "FROM coffee_sales cs\n",
    "JOIN product p ON cs.product_detail = p.product_detail\n",
    "JOIN location_sales ls ON CAST(cs.store_id AS VARCHAR) = ls.store\n",
    "JOIN date_sales ds ON cs.transaction_date::DATE = ds.full_date\n",
    "JOIN season s ON cs.season = s.name;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Exécution du script complet\n",
    "connection = None\n",
    "try:\n",
    "    # Établir la connexion\n",
    "    connection = engine.connect()\n",
    "    \n",
    "    # Démarrer une transaction\n",
    "    transaction = connection.begin()\n",
    "    \n",
    "    # 1. Création des tables\n",
    "    creation_order = [\n",
    "        table_product_categorie,\n",
    "        table_product_type,\n",
    "        table_location_sales,\n",
    "        table_date_sales,\n",
    "        table_season,\n",
    "        table_product,\n",
    "        table_fact_sales\n",
    "    ]\n",
    "    \n",
    "    for query in creation_order:\n",
    "        connection.execute(query)\n",
    "    \n",
    "    # 2. Insertion des données\n",
    "    insertion_order = [\n",
    "        insert_product_categorie,\n",
    "        insert_product_type,\n",
    "        insert_location_sales,\n",
    "        insert_date_sales,\n",
    "        insert_season,\n",
    "        insert_product,\n",
    "        insert_fact_sales\n",
    "    ]\n",
    "    \n",
    "    for query in insertion_order:\n",
    "        connection.execute(query)\n",
    "    \n",
    "    # 3. Création des index (après l'insertion pour optimiser les performances)\n",
    "    connection.execute(text(\"\"\"\n",
    "    -- Index pour les clés étrangères fréquemment utilisées\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_product ON fact_sales(product_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_location ON fact_sales(location_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_date ON fact_sales(date_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_season ON fact_sales(season_id);\n",
    "    \n",
    "    -- Index pour les requêtes combinant produit et date\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_composite ON fact_sales(product_id, date_id);\n",
    "    \n",
    "    -- Mise à jour des statistiques pour l'optimiseur de requêtes\n",
    "    ANALYZE fact_sales;\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Valider la transaction\n",
    "    transaction.commit()\n",
    "    print(\"Operation completed successfully: tables created, data loaded, and indexes optimized\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Annuler la transaction en cas d'erreur\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    if connection:\n",
    "        transaction.rollback()\n",
    "finally:\n",
    "    # Fermer la connexion\n",
    "    if connection:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a7f663",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae907a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Debut de l'execution du cript de téléchargement des données\")\n",
    "print('\\n')\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "df = pd.read_excel(\"coffe_sales_.xlsx\")\n",
    "print('shape :',df.shape)\n",
    "colonnes = df.columns.to_list()\n",
    "print(f\"{len(colonnes)}colonnes : {colonnes}\")\n",
    "df.head()\n",
    "\n",
    "print(\"Fin de l'execution du script de téléchargement des données\")\n",
    "print('\\n')\n",
    "print(\"\\n\")\n",
    "print(\"=====================================================================================================================================\")\n",
    "print(\"=====================================================================================================================================\")\n",
    "\n",
    "\n",
    "print(\"Debut de la creation de la base de donné coffe_sales\")\n",
    "print('\\n')\n",
    "\n",
    "# Connexion à la base postgres (par défaut pour créer une autre base)\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    port=\"5432\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Vérifie si la BDD existe déjà\n",
    "cur.execute(\"SELECT 1 FROM pg_database WHERE datname = 'coffee_sales'\")\n",
    "exists = cur.fetchone()\n",
    "\n",
    "if not exists:\n",
    "    cur.execute(\"CREATE DATABASE coffee_sales ENCODING 'LATIN1';\")\n",
    "    print(\"Base 'coffee_sales' créée.\")\n",
    "else:\n",
    "    print(\"La base 'coffee_sales' existe déjà.\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Connexion à la base coffee_sales\n",
    "comm = psycopg2.connect(\n",
    "    dbname=\"coffee_sales\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "cur = comm.cursor()\n",
    "cur.execute(\"SELECT 1\")\n",
    "if cur.fetchone():\n",
    "    print(\"Connexion à la base 'coffee_sales' réussie.\")\n",
    "cur.close()\n",
    "comm.close()\n",
    "\n",
    "# Création de la table à partir du DataFrame\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "## Envoi du DataFrame dans la BDD\n",
    "df.to_sql(\n",
    "    'coffee_sales', \n",
    "    engine, if_exists='replace',\n",
    "    chunksize=1700, # insertion par bloc de 1700 lignes pour éviter les erreurs de mémoire\n",
    "    index=False\n",
    ")\n",
    "print(\"Table 'coffee_sales' créée avec succès.\")\n",
    "\n",
    "\n",
    "query = '''select * from coffee_sales;'''\n",
    "tables_check = \"\"\" SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\"SELECT * FROM coffee_sales\", engine)\n",
    "tables_check =pd.read_sql(tables_check, engine)\n",
    "display(tables_check)\n",
    "dfcopy = df.copy()\n",
    "dfcopy.head()\n",
    "\n",
    "print(\"Fin de l'execution du script de cration de la base de connée coffee_sales\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"=====================================================================================================================================\")\n",
    "print(\"=====================================================================================================================================\")\n",
    "\n",
    "\n",
    "print(\"Déburt du nettoyage des donnée et enrichissement du dataframe\")\n",
    "\n",
    "import locale\n",
    "import time\n",
    "locale.setlocale(locale.LC_TIME, 'en_EN.UTF-8')\n",
    "from geopy.geocoders import Nominatim\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"coffe_sales_.xlsx\")\n",
    "print('nombre de lignes du csv brute :',df.shape)\n",
    "colonnes = df.columns.to_list()\n",
    "print(f\"{len(colonnes)}colonnes : {colonnes}\")\n",
    "df.head()\n",
    "\n",
    "# Connexion à la base postgres (par défaut pour créer une autre base)\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    port=\"5432\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Vérifie si la BDD existe déjà\n",
    "cur.execute(\"SELECT 1 FROM pg_database WHERE datname = 'coffee_sales'\")\n",
    "exists = cur.fetchone()\n",
    "\n",
    "if not exists:\n",
    "    cur.execute(\"CREATE DATABASE coffee_sales ENCODING 'LATIN1';\")\n",
    "    print(\"Base 'coffee_sales' créée.\")\n",
    "else:\n",
    "    print(\"La base 'coffee_sales' existe déjà.\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# instance de Connexion à la base coffee_sales\n",
    "comm = psycopg2.connect(\n",
    "    dbname=\"coffee_sales\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "cur = comm.cursor()\n",
    "cur.execute(\"SELECT 1\")\n",
    "if cur.fetchone():\n",
    "    print(\"Connexion à la base 'coffee_sales' réussie.\")\n",
    "cur.close()\n",
    "comm.close()\n",
    "\n",
    "# Création de la table à partir du DataFrame\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "## Envoi du DataFrame dans la BDD\n",
    "df.to_sql(\n",
    "    'coffee_sales', \n",
    "    engine, if_exists='replace',\n",
    "    chunksize=1700, # insertion par bloc de 1700 lignes pour éviter les erreurs de mémoire\n",
    "    index=False\n",
    ")\n",
    "print(\"Table 'coffee_sales' créée avec succès.\")\n",
    "\n",
    "\n",
    "query = '''select * from coffee_sales;'''\n",
    "tables_check = \"\"\" SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\"SELECT * FROM coffee_sales\", engine)\n",
    "tables_check =pd.read_sql(tables_check, engine)\n",
    "display(tables_check)\n",
    "dfcopy = df.copy() # on cré une copie de notre df pour avoir un df 'backup' des données brutes\n",
    "\n",
    "# oncré une colonne \"recipe\" qui nous aidera à calculer la recettes\n",
    "\n",
    "dfcopy[\"recipe\"]= dfcopy[\"transaction_qty\"] * dfcopy[\"unit_price\"] \n",
    "\n",
    "# convertissons la colonne transaction_date en datetime\n",
    "\n",
    "dfcopy[\"transaction_date\"] = pd.to_datetime(dfcopy[\"transaction_date\"])\n",
    "#dfcopy[\"transaction_time\"] = pd.to_datetime(dfcopy[\"transaction_time\"])\n",
    "\n",
    "# on cré les colonne temporelles pour affiné nos métrique\n",
    "\n",
    "dfcopy[\"hour\"] = pd.to_datetime(dfcopy[\"transaction_time\"], format=\"%H:%M:%S\").dt.hour# heure de la journée\n",
    "dfcopy[\"day\"] = dfcopy[\"transaction_date\"].dt.day # jour du mois\n",
    "dfcopy[\"month\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%B\") # mois de l'année\n",
    "dfcopy[\"weekday\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%A\") # jour de la semaine\n",
    "dfcopy[\"year\"]=dfcopy[\"transaction_date\"].dt.year \n",
    "\n",
    "# on cree une colonne  avec la date complète pour faciliter l'analyse et on supprimme l'ancienne colonne transaction_date\n",
    "\n",
    "dfcopy['full_date'] = dfcopy['transaction_date'].dt.strftime('%A, %#d %B %Y') +' '+ dfcopy['transaction_time'].astype(str)\n",
    "time.sleep(1)\n",
    "\n",
    "# on cré une colonne de la tranche horaire\n",
    "def get_tranche_horaire(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"matin\"\n",
    "    elif 12 <= hour < 14:\n",
    "        return \"midi\"\n",
    "    elif 14 <= hour < 18:\n",
    "        return \"après-midi\"\n",
    "    elif 18 <= hour < 22:\n",
    "        return \"soir\"\n",
    "    else:\n",
    "        return \"nuit\"\n",
    "\n",
    "dfcopy[\"time_slot\"] = dfcopy[\"hour\"].apply(get_tranche_horaire)\n",
    "\n",
    "# cré une colonne pour la saison pour une métrique plus poussée\n",
    "\n",
    "spring   = [\"March\", \"April\", \"May\"] #pritemps\n",
    "summer   = [\"June\", \"July\", \"August\"] # été\n",
    "autumn   = [\"September\", \"October\", \"November\"] # automne\n",
    "winter   = [\"December\", \"January\", \"February\"] #hiver\n",
    "\n",
    "dfcopy[\"season\"]= \"\"\n",
    "\n",
    "def get_saison(month):\n",
    "    if month in spring:\n",
    "        return \"spring\"\n",
    "    if month in summer:\n",
    "        return \"summer\"\n",
    "    if month in autumn:\n",
    "        return autumn\n",
    "    else:\n",
    "        return \"winter\"\n",
    "    \n",
    "dfcopy[\"season\"]=dfcopy[\"month\"].apply(get_saison)\n",
    "\n",
    "# on reorganise nos lignes par \"id_transaction\" ordre croissant\n",
    "\n",
    "dfcopy = dfcopy.sort_values(by=\"product_id\", ascending=True)\n",
    "\n",
    "# On cré deux colonnes 'Longitude' et 'lattitude' et on enrichit avec des données externes, pour une carto graphie\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Exemple : liste unique des stores depuis ton dataframe\n",
    "unique_stores = dfcopy['store_location'].dropna().unique()\n",
    "\n",
    "# Initialise Nominatim\n",
    "geolocator = Nominatim(user_agent=\"metabase_mapping\")\n",
    "\n",
    "# Stockage des résultats\n",
    "locations = []\n",
    "\n",
    "for store in unique_stores:\n",
    "    try:\n",
    "        # Utilise un nom de ville + \"New York\" ou autre si nécessaire\n",
    "        location = geolocator.geocode(store + \", New York, USA\", timeout=10)\n",
    "        if location:\n",
    "            print(f\"{store} => {location.latitude}, {location.longitude}\")\n",
    "            locations.append({\n",
    "                \"store_name\": store,\n",
    "                \"latitude\": location.latitude,\n",
    "                \"longitude\": location.longitude\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Pas trouvé : {store}\")\n",
    "            locations.append({\n",
    "                \"store_name\": store,\n",
    "                \"latitude\": None,\n",
    "                \"longitude\": None\n",
    "            })\n",
    "        time.sleep(1)  # Pour éviter de se faire bloquer par l'API\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {store}: {e}\")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "store_geo_df = pd.DataFrame(locations)\n",
    "\n",
    "# Visualisation ou export\n",
    "print(store_geo_df.head())\n",
    "store_geo_df.to_csv(\"store_locations_with_geo.csv\", index=False)\n",
    "\n",
    "coord_dict = {\n",
    "    \"Lower Manhattan\": [\"40.7135482\", \"-74.0054261\"],\n",
    "    \"Hell's Kitchen\": [\"40.7644228\", \"-73.9923918\"],\n",
    "    \"Astoria\": [\"40.7720145\", \"-73.9302673\"]\n",
    "}\n",
    "\n",
    "dfcopy[[\"latitude\", \"longitude\"]] = pd.DataFrame(dfcopy[\"store_location\"].map(coord_dict).tolist(), index=dfcopy.index)\n",
    "# Conversion des colonnes latitude et longitude en float (avec gestion des erreurs)\n",
    "dfcopy['latitude'] = pd.to_numeric(dfcopy['latitude'], errors='coerce')\n",
    "dfcopy['longitude'] = pd.to_numeric(dfcopy['longitude'], errors='coerce') #convertir la longitude et la latitude en float pour une visu map\n",
    "dfcopy['recipe'] = pd.to_numeric(dfcopy['recipe'], errors='coerce')\n",
    "dfcopy = dfcopy.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "dfcopy.to_sql(\n",
    "    'coffee_sales', \n",
    "    engine, if_exists='replace',\n",
    "    chunksize=1700, # insertion par bloc de 1700 lignes pour éviter les erreurs de mémoire\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Reorganisons notre dataframe \n",
    "\n",
    "colonnes = ['product_id','product_detail','product_category','product_type',\n",
    "            'unit_price','transaction_id','transaction_qty', 'transaction_date', \n",
    "            'transaction_time','store_id', 'store_location', 'recipe', \n",
    "            'weekday', 'day', 'month', 'hour', 'time_slot', 'season','year','full_date', 'latitude','longitude']\n",
    "dfcopy = dfcopy[colonnes]\n",
    "print((\"\\n\"))\n",
    "print(f\"Dataframe nettoyé {dfcopy.shape[0]} lignes  & {dfcopy.shape[1]} colonnes\")\n",
    "print(\"fin du nettoyage et insertion des du cvs nettoyé en base\")\n",
    "print(\"=====================================================================================================================================\")\n",
    "print(\"=====================================================================================================================================\")\n",
    "\n",
    "print(\"Debut de creation des tables dimentionnelle et insertions des données dans celles ci\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Configuration de la connexion à la base de données\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "# Requêtes de création des tables (dimensions -> faits)\n",
    "table_product_categorie = text(\"\"\"CREATE TABLE IF NOT EXISTS product_categorie (\n",
    "  id_categorie SERIAL PRIMARY KEY,\n",
    "  categorie VARCHAR(50) UNIQUE\n",
    ");\"\"\")\n",
    "\n",
    "table_product_type = text(\"\"\"CREATE TABLE IF NOT EXISTS product_type (\n",
    "  id_type SERIAL PRIMARY KEY,\n",
    "  type VARCHAR(50) UNIQUE\n",
    ");\"\"\")\n",
    "\n",
    "table_location_sales = text(\"\"\"CREATE TABLE IF NOT EXISTS location_sales (\n",
    "  id_location SERIAL PRIMARY KEY,\n",
    "  location VARCHAR(100) UNIQUE,\n",
    "  latitude FLOAT,\n",
    "  longitude FLOAT\n",
    ");\"\"\")\n",
    "\n",
    "table_date_sales = text(\"\"\"CREATE TABLE IF NOT EXISTS date_sales (\n",
    "  id_date SERIAL PRIMARY KEY,\n",
    "  full_date DATE UNIQUE,\n",
    "  day INT,\n",
    "  weekday VARCHAR(50),\n",
    "  month VARCHAR(50),\n",
    "  year INT,\n",
    "  hour INT,\n",
    "  time_slot VARCHAR(25)\n",
    ");\"\"\")\n",
    "\n",
    "table_season = text(\"\"\"CREATE TABLE IF NOT EXISTS season (\n",
    "  id_season SERIAL PRIMARY KEY,\n",
    "  name VARCHAR(50) UNIQUE\n",
    ");\"\"\")\n",
    "\n",
    "table_product = text(\"\"\"CREATE TABLE IF NOT EXISTS product (\n",
    "  product_id SERIAL PRIMARY KEY,\n",
    "  product_detail VARCHAR(255) UNIQUE,\n",
    "  id_categorie INT REFERENCES product_categorie(id_categorie),\n",
    "  id_type INT REFERENCES product_type(id_type),\n",
    "  unit_price DECIMAL(10,2)\n",
    ");\"\"\")\n",
    "\n",
    "table_fact_sales = text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_sales (\n",
    "  id_transaction INT PRIMARY KEY,  -- on récupère l'ID existant de coffee_sales\n",
    "  product_id INT REFERENCES product(product_id),\n",
    "  location_id INT REFERENCES location_sales(id_location),\n",
    "  date_id INT REFERENCES date_sales(id_date),\n",
    "  season_id INT REFERENCES season(id_season),\n",
    "  quantity INT,\n",
    "  unit_price DECIMAL(10,2)\n",
    ");\"\"\")\n",
    "\n",
    "\n",
    "# Requêtes d'insertion des données\n",
    "insert_product_categorie = text(\"\"\"\n",
    "INSERT INTO product_categorie (categorie)\n",
    "SELECT DISTINCT product_category FROM coffee_sales\n",
    "ON CONFLICT (categorie) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_product_type = text(\"\"\"\n",
    "INSERT INTO product_type (type)\n",
    "SELECT DISTINCT product_type FROM coffee_sales\n",
    "ON CONFLICT (type) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_location_sales = text(\"\"\"\n",
    "INSERT INTO location_sales (store, latitude, longitude)\n",
    "SELECT DISTINCT store_location, latitude, longitude FROM coffee_sales\n",
    "ON CONFLICT (store) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_date_sales = text(\"\"\"\n",
    "INSERT INTO date_sales (full_date, day, weekday, month, year, hour, time_slot)\n",
    "SELECT DISTINCT \n",
    "    transaction_date::DATE,\n",
    "    day,\n",
    "    weekday,\n",
    "    month,\n",
    "    year,\n",
    "    hour,\n",
    "    time_slot\n",
    "FROM coffee_sales\n",
    "ON CONFLICT (full_date) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_season = text(\"\"\"\n",
    "INSERT INTO season (name)\n",
    "SELECT DISTINCT season FROM coffee_sales\n",
    "ON CONFLICT (name) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_product = text(\"\"\"\n",
    "INSERT INTO product (product_detail, id_categorie, id_type, unit_price)\n",
    "SELECT DISTINCT \n",
    "    cs.product_detail,\n",
    "    pc.id_categorie,\n",
    "    pt.id_type,\n",
    "    cs.unit_price\n",
    "FROM coffee_sales cs\n",
    "JOIN product_categorie pc ON cs.product_category = pc.categorie\n",
    "JOIN product_type pt ON cs.product_type = pt.type\n",
    "ON CONFLICT (product_detail) DO NOTHING;\"\"\")\n",
    "\n",
    "insert_fact_sales = text(\"\"\"\n",
    "INSERT INTO fact_sales (\n",
    "    id_transaction,\n",
    "    product_id,\n",
    "    location_id,\n",
    "    date_id,\n",
    "    season_id,\n",
    "    quantity,\n",
    "    unit_price\n",
    ")\n",
    "SELECT \n",
    "    cs.transaction_id,\n",
    "    p.product_id,\n",
    "    ls.id_location,\n",
    "    ds.id_date,\n",
    "    s.id_season,\n",
    "    cs.transaction_qty,\n",
    "    cs.unit_price\n",
    "FROM coffee_sales cs\n",
    "JOIN product p ON cs.product_detail = p.product_detail\n",
    "JOIN location_sales ls ON CAST(cs.store_id AS VARCHAR) = ls.store\n",
    "JOIN date_sales ds ON cs.transaction_date::DATE = ds.full_date\n",
    "JOIN season s ON cs.season = s.name;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Exécution du script complet\n",
    "connection = None\n",
    "try:\n",
    "    # Établir la connexion\n",
    "    connection = engine.connect()\n",
    "    \n",
    "    # Démarrer une transaction\n",
    "    transaction = connection.begin()\n",
    "    \n",
    "    # 1. Création des tables\n",
    "    creation_order = [\n",
    "        table_product_categorie,\n",
    "        table_product_type,\n",
    "        table_location_sales,\n",
    "        table_date_sales,\n",
    "        table_season,\n",
    "        table_product,\n",
    "        table_fact_sales\n",
    "    ]\n",
    "    \n",
    "    for query in creation_order:\n",
    "        connection.execute(query)\n",
    "    \n",
    "    # 2. Insertion des données\n",
    "    insertion_order = [\n",
    "        insert_product_categorie,\n",
    "        insert_product_type,\n",
    "        insert_location_sales,\n",
    "        insert_date_sales,\n",
    "        insert_season,\n",
    "        insert_product,\n",
    "        insert_fact_sales\n",
    "    ]\n",
    "    \n",
    "    for query in insertion_order:\n",
    "        connection.execute(query)\n",
    "    \n",
    "    # 3. Création des index (après l'insertion pour optimiser les performances)\n",
    "    connection.execute(text(\"\"\"\n",
    "    -- Index pour les clés étrangères fréquemment utilisées\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_product ON fact_sales(product_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_location ON fact_sales(location_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_date ON fact_sales(date_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_sales_season ON fact_sales(season_id);\n",
    "    \n",
    "    -- Index pour les requêtes combinant produit et date\n",
    "    CREATE INDEX IF NOT EXISTS idx_fact_composite ON fact_sales(product_id, date_id);\n",
    "    \n",
    "    -- Mise à jour des statistiques pour l'optimiseur de requêtes\n",
    "    ANALYZE fact_sales;\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Valider la transaction\n",
    "    transaction.commit()\n",
    "    print(\"Operation completed successfully: tables created, data loaded, and indexes optimized\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Annuler la transaction en cas d'erreur\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    if connection:\n",
    "        transaction.rollback()\n",
    "finally:\n",
    "    # Fermer la connexion\n",
    "    if connection:\n",
    "        connection.close()\n",
    "\n",
    "\n",
    "print(\"=====================================================================================================================================\")\n",
    "print(\"=================================================== FIN D'EXECUTION  DU SCRIPT COMPLET ===============================\")\n",
    "print(\"=====================================================================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Début de l'exécution du script de téléchargement des données\\n\")\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Chargement des données\n",
    "df = pd.read_excel(\"coffe_sales_.xlsx\")\n",
    "print('Shape :', df.shape)\n",
    "print(f\"{len(df.columns)} colonnes : {df.columns.tolist()}\")\n",
    "\n",
    "print(\"Fin de l'exécution du script de téléchargement des données\\n\")\n",
    "print(\"=\"*150)\n",
    "\n",
    "print(\"Début de la création de la base de données 'coffee_sales'\\n\")\n",
    "\n",
    "# Connexion à PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1212\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Création BDD si elle n'existe pas\n",
    "cur.execute(\"SELECT 1 FROM pg_database WHERE datname = 'coffee_sales'\")\n",
    "if not cur.fetchone():\n",
    "    cur.execute(\"CREATE DATABASE coffee_sales ENCODING 'LATIN1';\")\n",
    "    print(\"Base 'coffee_sales' créée.\")\n",
    "else:\n",
    "    print(\"La base 'coffee_sales' existe déjà.\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Connexion à la base coffee_sales\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "\n",
    "# Envoi du DataFrame en base\n",
    "df.to_sql('coffee_sales', engine, if_exists='replace', chunksize=1700, index=False)\n",
    "print(\"Table 'coffee_sales' créée avec succès.\")\n",
    "\n",
    "print(\"Fin de la création de la base de données\\n\")\n",
    "print(\"=\"*150)\n",
    "\n",
    "print(\"Début du nettoyage et enrichissement des données\\n\")\n",
    "\n",
    "dfcopy = df.copy()\n",
    "dfcopy[\"recipe\"] = dfcopy[\"transaction_qty\"] * dfcopy[\"unit_price\"]\n",
    "dfcopy[\"transaction_date\"] = pd.to_datetime(dfcopy[\"transaction_date\"])\n",
    "dfcopy[\"hour\"] = pd.to_datetime(dfcopy[\"transaction_time\"], format=\"%H:%M:%S\").dt.hour\n",
    "dfcopy[\"day\"] = dfcopy[\"transaction_date\"].dt.day\n",
    "dfcopy[\"month\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%B\")\n",
    "dfcopy[\"weekday\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%A\")\n",
    "dfcopy[\"year\"] = dfcopy[\"transaction_date\"].dt.year\n",
    "dfcopy['full_date'] = dfcopy['transaction_date'].dt.strftime('%A, %#d %B %Y') + ' ' + dfcopy['transaction_time'].astype(str)\n",
    "\n",
    "def get_tranche_horaire(hour):\n",
    "    if 5 <= hour < 12: return \"matin\"\n",
    "    if 12 <= hour < 14: return \"midi\"\n",
    "    if 14 <= hour < 18: return \"après-midi\"\n",
    "    if 18 <= hour < 22: return \"soir\"\n",
    "    return \"nuit\"\n",
    "dfcopy[\"time_slot\"] = dfcopy[\"hour\"].apply(get_tranche_horaire)\n",
    "\n",
    "def get_saison(month):\n",
    "    if month in [\"March\", \"April\", \"May\"]: return \"spring\"\n",
    "    if month in [\"June\", \"July\", \"August\"]: return \"summer\"\n",
    "    if month in [\"September\", \"October\", \"November\"]: return \"autumn\"\n",
    "    return \"winter\"\n",
    "dfcopy[\"season\"] = dfcopy[\"month\"].apply(get_saison)\n",
    "\n",
    "dfcopy = dfcopy.sort_values(by=\"product_id\", ascending=True)\n",
    "\n",
    "coord_dict = {\n",
    "    \"Lower Manhattan\": [\"40.7135482\", \"-74.0054261\"],\n",
    "    \"Hell's Kitchen\": [\"40.7644228\", \"-73.9923918\"],\n",
    "    \"Astoria\": [\"40.7720145\", \"-73.9302673\"]\n",
    "}\n",
    "dfcopy[[\"latitude\", \"longitude\"]] = pd.DataFrame(dfcopy[\"store_location\"].map(coord_dict).tolist(), index=dfcopy.index)\n",
    "dfcopy['latitude'] = pd.to_numeric(dfcopy['latitude'], errors='coerce')\n",
    "dfcopy['longitude'] = pd.to_numeric(dfcopy['longitude'], errors='coerce')\n",
    "dfcopy['recipe'] = pd.to_numeric(dfcopy['recipe'], errors='coerce')\n",
    "dfcopy = dfcopy.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "dfcopy.to_sql('coffee_sales', engine, if_exists='replace', chunksize=1700, index=False)\n",
    "\n",
    "colonnes = ['product_id','product_detail','product_category','product_type',\n",
    "            'unit_price','transaction_id','transaction_qty', 'transaction_date', \n",
    "            'transaction_time', 'store_location', 'recipe', \n",
    "            'weekday', 'day', 'month', 'hour', 'time_slot', 'season','year','full_date', 'latitude','longitude']\n",
    "dfcopy = dfcopy[colonnes]\n",
    "print(f\"\\nDataframe nettoyé : {dfcopy.shape[0]} lignes & {dfcopy.shape[1]} colonnes\")\n",
    "print(\"Fin du nettoyage des données\\n\")\n",
    "print(\"=\"*150)\n",
    "\n",
    "print(\"Début de la création des tables dimensionnelles et des insertions\\n\")\n",
    "\n",
    "# Requêtes de création de tables\n",
    "create_statements = [\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS product_categorie (\n",
    "      id_categorie SERIAL PRIMARY KEY,\n",
    "      categorie VARCHAR(50) UNIQUE\n",
    "    );\"\"\"),\n",
    "\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS product_type (\n",
    "      id_type SERIAL PRIMARY KEY,\n",
    "      type VARCHAR(50) UNIQUE\n",
    "    );\"\"\"),\n",
    "\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS location_sales (\n",
    "      id_location SERIAL PRIMARY KEY,\n",
    "      store VARCHAR(100) UNIQUE,\n",
    "      latitude FLOAT,\n",
    "      longitude FLOAT\n",
    "    );\"\"\"),\n",
    "\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS date_sales (\n",
    "      id_date SERIAL PRIMARY KEY,\n",
    "      full_date DATE UNIQUE,\n",
    "      day INT,\n",
    "      weekday VARCHAR(50),\n",
    "      month VARCHAR(50),\n",
    "      year INT,\n",
    "      hour INT,\n",
    "      time_slot VARCHAR(25)\n",
    "    );\"\"\"),\n",
    "\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS season (\n",
    "      id_season SERIAL PRIMARY KEY,\n",
    "      name VARCHAR(50) UNIQUE\n",
    "    );\"\"\"),\n",
    "\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS product (\n",
    "      product_id SERIAL PRIMARY KEY,\n",
    "      product_detail VARCHAR(255) UNIQUE,\n",
    "      id_categorie INT REFERENCES product_categorie(id_categorie),\n",
    "      id_type INT REFERENCES product_type(id_type),\n",
    "      unit_price DECIMAL(10,2)\n",
    "    );\"\"\"),\n",
    "\n",
    "    text(\"\"\"CREATE TABLE IF NOT EXISTS fact_sales (\n",
    "      id_transaction INT PRIMARY KEY,\n",
    "      product_id INT REFERENCES product(product_id),\n",
    "      location_id INT REFERENCES location_sales(id_location),\n",
    "      date_id INT REFERENCES date_sales(id_date),\n",
    "      season_id INT REFERENCES season(id_season),\n",
    "      quantity INT,\n",
    "      unit_price DECIMAL(10,2)\n",
    "    );\"\"\")\n",
    "]\n",
    "\n",
    "# Requêtes d'insertion\n",
    "insert_statements = [\n",
    "    text(\"\"\"INSERT INTO product_categorie (categorie)\n",
    "        SELECT DISTINCT product_category FROM coffee_sales\n",
    "        ON CONFLICT (categorie) DO NOTHING;\"\"\"),\n",
    "\n",
    "    text(\"\"\"INSERT INTO product_type (type)\n",
    "        SELECT DISTINCT product_type FROM coffee_sales\n",
    "        ON CONFLICT (type) DO NOTHING;\"\"\"),\n",
    "\n",
    "    text(\"\"\"INSERT INTO location_sales (store, latitude, longitude)\n",
    "        SELECT DISTINCT store_id, latitude, longitude FROM coffee_sales\n",
    "        ON CONFLICT (store) DO NOTHING;\"\"\"),\n",
    "\n",
    "    text(\"\"\"INSERT INTO date_sales (full_date, day, weekday, month, year, hour, time_slot)\n",
    "        SELECT DISTINCT \n",
    "            transaction_date::DATE, day, weekday, month, year, hour, time_slot\n",
    "        FROM coffee_sales\n",
    "        ON CONFLICT (full_date) DO NOTHING;\"\"\"),\n",
    "\n",
    "    text(\"\"\"INSERT INTO season (name)\n",
    "        SELECT DISTINCT season FROM coffee_sales\n",
    "        ON CONFLICT (name) DO NOTHING;\"\"\"),\n",
    "\n",
    "    text(\"\"\"INSERT INTO product (product_detail, id_categorie, id_type, unit_price)\n",
    "        SELECT DISTINCT \n",
    "            cs.product_detail,\n",
    "            pc.id_categorie,\n",
    "            pt.id_type,\n",
    "            cs.unit_price\n",
    "        FROM coffee_sales cs\n",
    "        JOIN product_categorie pc ON cs.product_category = pc.categorie\n",
    "        JOIN product_type pt ON cs.product_type = pt.type\n",
    "        ON CONFLICT (product_detail) DO NOTHING;\"\"\"),\n",
    "\n",
    "    text(\"\"\"INSERT INTO fact_sales (\n",
    "        id_transaction,\n",
    "        product_id,\n",
    "        location_id,\n",
    "        date_id,\n",
    "        season_id,\n",
    "        quantity,\n",
    "        unit_price\n",
    "    )\n",
    "    SELECT \n",
    "        cs.transaction_id,\n",
    "        p.product_id,\n",
    "        ls.id_location,\n",
    "        ds.id_date,\n",
    "        s.id_season,\n",
    "        cs.transaction_qty,\n",
    "        cs.unit_price\n",
    "    FROM coffee_sales cs\n",
    "    JOIN product p ON cs.product_detail = p.product_detail\n",
    "    JOIN location_sales ls ON CAST(cs.location AS VARCHAR) = ls.store\n",
    "    JOIN date_sales ds ON cs.transaction_date::DATE = ds.full_date\n",
    "    JOIN season s ON cs.season = s.name;\"\"\")\n",
    "]\n",
    "\n",
    "# Exécution des requêtes\n",
    "with engine.connect() as connection:\n",
    "    transaction = connection.begin()\n",
    "    try:\n",
    "        for stmt in create_statements:\n",
    "            connection.execute(stmt)\n",
    "\n",
    "        for stmt in insert_statements:\n",
    "            connection.execute(stmt)\n",
    "\n",
    "        connection.execute(text(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_sales_product ON fact_sales(product_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_sales_location ON fact_sales(location_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_sales_date ON fact_sales(date_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_sales_season ON fact_sales(season_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_composite ON fact_sales(product_id, date_id);\n",
    "        ANALYZE fact_sales;\n",
    "        \"\"\"))\n",
    "\n",
    "        transaction.commit()\n",
    "        print(\"Opération réussie : tables créées, données insérées, index optimisés\")\n",
    "\n",
    "    except Exception as e:\n",
    "        transaction.rollback()\n",
    "        print(f\"Erreur : {e}\")\n",
    "\n",
    "print(\"=\"*150)\n",
    "print(\"========================= FIN D'EXÉCUTION DU SCRIPT =========================\")\n",
    "print(\"=\"*150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b76e9",
   "metadata": {},
   "source": [
    "# Code complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ca22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "import locale\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "print(\"Debut de l'execution du script de téléchargement des données...\")\n",
    "\n",
    "# Téléchargement et affichage du jeu de données \"Coffee Sales\" depuis Kaggle\n",
    "# Assurez-vous d'avoir installé kagglehub avec `pip install kagglehub`\n",
    "def download_dataset(path):\n",
    "    df = None\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(\"ahmedabbas757/coffee-sales\")\n",
    "        \n",
    "        files = os.listdir(path)\n",
    "        excel_files = [f for f in files if f.endswith('.xlsx')]\n",
    "        \n",
    "        if excel_files:\n",
    "            df = pd.read_excel(f\"{path}/{excel_files[0]}\")\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            print(\"Aucun fichier CSV trouvé dans le dossier téléchargé.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "download_dataset(\"coffee_sales\")\n",
    "\n",
    "\n",
    "df = download_dataset(\"coffee_sales\")\n",
    "\n",
    "print(\"Fin de l'execution du script de téléchargement des données\")\n",
    "print(\"\\n=====================================================================================================================================\")\n",
    "print(\"Debut de la creation de la base de donné coffee_sales\")\n",
    "\n",
    "# Création de la base si elle n'existe pas\n",
    "conn = psycopg2.connect(host=\"localhost\", dbname=\"postgres\", user=\"postgres\", password=\"1212\", port=\"5432\")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT 1 FROM pg_database WHERE datname = 'coffee_sales'\")\n",
    "if not cur.fetchone():\n",
    "    cur.execute(\"CREATE DATABASE coffee_sales ENCODING 'UTF8';\")\n",
    "    print(\"Base 'coffee_sales' créée.\")\n",
    "else:\n",
    "    print(\"La base 'coffee_sales' existe déjà.\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Connexion avec SQLAlchemy\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1212@localhost:5432/coffee_sales')\n",
    "df.to_sql('coffee_sales', engine, if_exists='replace', chunksize=1700, index=False)\n",
    "print(\"Table 'coffee_sales' créée avec succès.\")\n",
    "\n",
    "print(\"\\nFin de l'execution du script de creation de la base de donnée coffee_sales\")\n",
    "print(\"\\n=====================================================================================================================================\")\n",
    "print(\"Début du nettoyage des données et enrichissement du dataframe\")\n",
    "\n",
    "# Nettoyage des données\n",
    "locale.setlocale(locale.LC_TIME, 'en_EN.UTF-8')\n",
    "dfcopy = df.copy()\n",
    "dfcopy[\"recipe\"] = dfcopy[\"transaction_qty\"] * dfcopy[\"unit_price\"]\n",
    "dfcopy[\"transaction_date\"] = pd.to_datetime(dfcopy[\"transaction_date\"])\n",
    "dfcopy[\"hour\"] = pd.to_datetime(dfcopy[\"transaction_time\"], format=\"%H:%M:%S\").dt.hour\n",
    "dfcopy[\"day\"] = dfcopy[\"transaction_date\"].dt.day\n",
    "dfcopy[\"month\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%B\")\n",
    "dfcopy[\"weekday\"] = dfcopy[\"transaction_date\"].dt.strftime(\"%A\")\n",
    "dfcopy[\"year\"] = dfcopy[\"transaction_date\"].dt.year\n",
    "dfcopy['full_date'] = dfcopy['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "def get_tranche_horaire(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"matin\"\n",
    "    elif 12 <= hour < 14:\n",
    "        return \"midi\"\n",
    "    elif 14 <= hour < 18:\n",
    "        return \"après-midi\"\n",
    "    elif 18 <= hour < 22:\n",
    "        return \"soir\"\n",
    "    else:\n",
    "        return \"nuit\"\n",
    "\n",
    "dfcopy[\"time_slot\"] = dfcopy[\"hour\"].apply(get_tranche_horaire)\n",
    "\n",
    "seasons = {\"March\": \"spring\", \"April\": \"spring\", \"May\": \"spring\",\n",
    "           \"June\": \"summer\", \"July\": \"summer\", \"August\": \"summer\",\n",
    "           \"September\": \"autumn\", \"October\": \"autumn\", \"November\": \"autumn\",\n",
    "           \"December\": \"winter\", \"January\": \"winter\", \"February\": \"winter\"}\n",
    "dfcopy[\"season\"] = dfcopy[\"month\"].map(seasons)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "unique_stores = dfcopy['store_location'].dropna().unique()\n",
    "locations = []\n",
    "\n",
    "for store in unique_stores:\n",
    "    try:\n",
    "        location = geolocator.geocode(store + \", New York, USA\", timeout=10)\n",
    "        if location:\n",
    "            locations.append({\"store_name\": store, \"latitude\": location.latitude, \"longitude\": location.longitude})\n",
    "        else:\n",
    "            locations.append({\"store_name\": store, \"latitude\": None, \"longitude\": None})\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {store}: {e}\")\n",
    "\n",
    "geo_df = pd.DataFrame(locations)\n",
    "coord_dict = geo_df.set_index(\"store_name\").to_dict(\"index\")\n",
    "dfcopy[[\"latitude\", \"longitude\"]] = pd.DataFrame(dfcopy[\"store_location\"].map(lambda x: coord_dict.get(x, {\"latitude\": None, \"longitude\": None})).tolist(), index=dfcopy.index)\n",
    "dfcopy['latitude'] = pd.to_numeric(dfcopy['latitude'], errors='coerce')\n",
    "dfcopy['longitude'] = pd.to_numeric(dfcopy['longitude'], errors='coerce')\n",
    "dfcopy['recipe'] = pd.to_numeric(dfcopy['recipe'], errors='coerce')\n",
    "dfcopy = dfcopy.dropna(subset=['latitude', 'longitude'])\n",
    "dfcopy.to_sql('coffee_sales', engine, if_exists='replace', chunksize=1700, index=False)\n",
    "\n",
    "print(f\"Dataframe nettoyé {dfcopy.shape[0]} lignes & {dfcopy.shape[1]} colonnes\")\n",
    "\n",
    "print(\"Fin du nettoyage et insertion des données nettoyées en base\")\n",
    "print(\"\\n=====================================================================================================================================\")\n",
    "print(\"Debut de creation des tables dimensionnelles et insertion\")\n",
    "\n",
    "# Creation des tables\n",
    "schemas = [\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS product_categorie (id_categorie SERIAL PRIMARY KEY, categorie VARCHAR(50) UNIQUE);\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS product_type (id_type SERIAL PRIMARY KEY, type VARCHAR(50) UNIQUE);\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS location_sales (id_location SERIAL PRIMARY KEY, store_location VARCHAR(100) UNIQUE, latitude FLOAT, longitude FLOAT);\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS date_sales (id_date SERIAL PRIMARY KEY, full_date DATE UNIQUE, day INT, weekday VARCHAR(50), month VARCHAR(50), year INT, hour INT, time_slot VARCHAR(25));\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS season (id_season SERIAL PRIMARY KEY, name VARCHAR(50) UNIQUE);\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS product (product_id SERIAL PRIMARY KEY, product_detail VARCHAR(255) UNIQUE, id_categorie INT REFERENCES product_categorie(id_categorie), id_type INT REFERENCES product_type(id_type), unit_price DECIMAL(10,2));\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS fact_sales (id_transaction INT PRIMARY KEY, product_id INT REFERENCES product(product_id), location_id INT REFERENCES location_sales(id_location), date_id INT REFERENCES date_sales(id_date), season_id INT REFERENCES season(id_season), quantity INT, unit_price DECIMAL(10,2));\"\"\"\n",
    "]\n",
    "\n",
    "inserts = [\n",
    "    \"\"\"INSERT INTO product_categorie (categorie) SELECT DISTINCT product_category FROM coffee_sales ON CONFLICT (categorie) DO NOTHING;\"\"\",\n",
    "    \"\"\"INSERT INTO product_type (type) SELECT DISTINCT product_type FROM coffee_sales ON CONFLICT (type) DO NOTHING;\"\"\",\n",
    "    \"\"\"INSERT INTO location_sales (store_location, latitude, longitude) SELECT DISTINCT store_location, latitude, longitude FROM coffee_sales ON CONFLICT (store_location) DO NOTHING;\"\"\",\n",
    "    \"\"\"INSERT INTO date_sales (full_date, day, weekday, month, year, hour, time_slot) SELECT DISTINCT transaction_date::DATE, day, weekday, month, year, hour, time_slot FROM coffee_sales ON CONFLICT (full_date) DO NOTHING;\"\"\",\n",
    "    \"\"\"INSERT INTO season (name) SELECT DISTINCT season FROM coffee_sales ON CONFLICT (name) DO NOTHING;\"\"\",\n",
    "    \"\"\"INSERT INTO product (product_detail, id_categorie, id_type, unit_price) SELECT DISTINCT cs.product_detail, pc.id_categorie, pt.id_type, cs.unit_price FROM coffee_sales cs JOIN product_categorie pc ON cs.product_category = pc.categorie JOIN product_type pt ON cs.product_type = pt.type ON CONFLICT (product_detail) DO NOTHING;\"\"\",\n",
    "    \"\"\"INSERT INTO fact_sales (id_transaction, product_id, location_id, date_id, season_id, quantity, unit_price) SELECT cs.transaction_id, p.product_id, ls.id_location, ds.id_date, s.id_season, cs.transaction_qty, cs.unit_price FROM coffee_sales cs JOIN product p ON cs.product_detail = p.product_detail JOIN location_sales ls ON CAST(cs.store_location AS VARCHAR) = ls.store_location JOIN date_sales ds ON cs.transaction_date::DATE = ds.full_date JOIN season s ON cs.season = s.name;\"\"\"\n",
    "]\n",
    "\n",
    "# creation des index (performance des requetes)\n",
    "with engine.connect() as conn:\n",
    "    trans = conn.begin()\n",
    "    try:\n",
    "        for schema in schemas:\n",
    "            conn.execute(text(schema))\n",
    "        for insert in inserts:\n",
    "            conn.execute(text(insert))\n",
    "\n",
    "        # Indexation\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_fact_sales_product ON fact_sales(product_id);\n",
    "            CREATE INDEX IF NOT EXISTS idx_fact_sales_location ON fact_sales(location_id);\n",
    "            CREATE INDEX IF NOT EXISTS idx_fact_sales_date ON fact_sales(date_id);\n",
    "            CREATE INDEX IF NOT EXISTS idx_fact_sales_season ON fact_sales(season_id);\n",
    "            CREATE INDEX IF NOT EXISTS idx_fact_composite ON fact_sales(product_id, date_id);\n",
    "            ANALYZE fact_sales;\n",
    "        \"\"\"))\n",
    "\n",
    "        trans.commit()\n",
    "        print(\"Operation completed successfully: tables created, data loaded, and indexes optimized\")\n",
    "    except Exception as e:\n",
    "        trans.rollback()\n",
    "        print(\"Error occurred:\", e)\n",
    "\n",
    "print(\"\\n=================================================== FIN D'EXECUTION DU SCRIPT COMPLET ===============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24650ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29a192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf5940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33aad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c48ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259282e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "old_name = 'About dataset.txt'\n",
    "new_name = 'consigne.txt'\n",
    "if os.path.exists(old_name):\n",
    "    os.rename(old_name, new_name)\n",
    "else:\n",
    "    print(f\"Le fichier {old_name} n'existe pas.\")\n",
    "consigne = pd.read_csv(\"consigne.txt\", sep=\"\\t\", header=None, names=[\"Consigne\"])\n",
    "consigne.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89296275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "dsn = \"host=localhost dbname=coffee_sales user=postgres password=1212 port=5432\"\n",
    "\n",
    "conn = psycopg2.connect(dsn)\n",
    "cur = conn.cursor()\n",
    "print(\"Connexion réussie\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc37e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "\n",
    "# 🔐 Connexion PostgreSQL\n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:<votre_mot_de_passe>@localhost:5432/coffee_sales\")\n",
    "conn = engine.connect()\n",
    "\n",
    "# 1️⃣ Chargement & enrichissement\n",
    "df = pd.read_csv(\"coffee_sales.csv\")\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "df['season'] = df['transaction_date'].dt.month.map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "})\n",
    "df['full_date'] = df['transaction_date'].dt.date\n",
    "\n",
    "# 2️⃣ Insertion dans table brute\n",
    "df.to_sql(\"coffee_sales\", engine, if_exists='replace', index=False)\n",
    "\n",
    "# 3️⃣ Création des dimensions\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product (\n",
    "    product_id SERIAL PRIMARY KEY,\n",
    "    product_detail TEXT UNIQUE\n",
    ");\n",
    "INSERT INTO product (product_detail)\n",
    "SELECT DISTINCT product_detail FROM coffee_sales\n",
    "ON CONFLICT DO NOTHING;\n",
    "\"\"\"))\n",
    "\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS location_sales (\n",
    "    id_location SERIAL PRIMARY KEY,\n",
    "    store TEXT UNIQUE,\n",
    "    store_location TEXT\n",
    ");\n",
    "INSERT INTO location_sales (store, store_location)\n",
    "SELECT DISTINCT store_id::TEXT, store_location FROM coffee_sales\n",
    "ON CONFLICT DO NOTHING;\n",
    "\"\"\"))\n",
    "\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS date_sales (\n",
    "    id_date SERIAL PRIMARY KEY,\n",
    "    full_date DATE UNIQUE,\n",
    "    day INT,\n",
    "    month INT,\n",
    "    year INT\n",
    ");\n",
    "INSERT INTO date_sales (full_date, day, month, year)\n",
    "SELECT DISTINCT full_date, EXTRACT(DAY FROM transaction_date),\n",
    "       EXTRACT(MONTH FROM transaction_date),\n",
    "       EXTRACT(YEAR FROM transaction_date)\n",
    "FROM coffee_sales\n",
    "ON CONFLICT DO NOTHING;\n",
    "\"\"\"))\n",
    "\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS season (\n",
    "    id_season SERIAL PRIMARY KEY,\n",
    "    name TEXT UNIQUE\n",
    ");\n",
    "INSERT INTO season (name)\n",
    "SELECT DISTINCT season FROM coffee_sales\n",
    "ON CONFLICT DO NOTHING;\n",
    "\"\"\"))\n",
    "\n",
    "# 4️⃣ Table de faits + insertion avec gestion des conflits\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_sales (\n",
    "    id_transaction INT PRIMARY KEY,\n",
    "    product_id INT REFERENCES product(product_id),\n",
    "    location_id INT REFERENCES location_sales(id_location),\n",
    "    date_id INT REFERENCES date_sales(id_date),\n",
    "    season_id INT REFERENCES season(id_season),\n",
    "    quantity INT,\n",
    "    unit_price DECIMAL(10,2),\n",
    "    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"))\n",
    "\n",
    "conn.execute(text(\"\"\"\n",
    "INSERT INTO fact_sales (\n",
    "    id_transaction,\n",
    "    product_id,\n",
    "    location_id,\n",
    "    date_id,\n",
    "    season_id,\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    created_at,\n",
    "    updated_at\n",
    ")\n",
    "SELECT \n",
    "    cs.transaction_id,\n",
    "    p.product_id,\n",
    "    ls.id_location,\n",
    "    ds.id_date,\n",
    "    s.id_season,\n",
    "    cs.transaction_qty,\n",
    "    cs.unit_price,\n",
    "    CURRENT_TIMESTAMP,\n",
    "    CURRENT_TIMESTAMP\n",
    "FROM coffee_sales cs\n",
    "JOIN product p ON cs.product_detail = p.product_detail\n",
    "JOIN location_sales ls ON CAST(cs.store_id AS TEXT) = ls.store\n",
    "JOIN date_sales ds ON cs.full_date = ds.full_date\n",
    "JOIN season s ON cs.season = s.name\n",
    "ON CONFLICT (id_transaction) DO UPDATE SET \n",
    "    product_id = EXCLUDED.product_id,\n",
    "    location_id = EXCLUDED.location_id,\n",
    "    date_id = EXCLUDED.date_id,\n",
    "    season_id = EXCLUDED.season_id,\n",
    "    quantity = EXCLUDED.quantity,\n",
    "    unit_price = EXCLUDED.unit_price,\n",
    "    updated_at = CURRENT_TIMESTAMP;\n",
    "\"\"\"))\n",
    "\n",
    "# 5️⃣ Journalisation des changements\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS audit_fact_sales (\n",
    "    audit_id SERIAL PRIMARY KEY,\n",
    "    id_transaction INT,\n",
    "    operation_type TEXT,\n",
    "    operation_timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"))\n",
    "\n",
    "conn.execute(text(\"\"\"\n",
    "CREATE OR REPLACE FUNCTION log_fact_sales_change() RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "    IF TG_OP = 'INSERT' THEN\n",
    "        INSERT INTO audit_fact_sales(id_transaction, operation_type)\n",
    "        VALUES (NEW.id_transaction, 'INSERT');\n",
    "    ELSIF TG_OP = 'UPDATE' THEN\n",
    "        INSERT INTO audit_fact_sales(id_transaction, operation_type)\n",
    "        VALUES (NEW.id_transaction, 'UPDATE');\n",
    "    END IF;\n",
    "    RETURN NEW;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\"\"\"))\n",
    "\n",
    "conn.execute(text(\"\"\"\n",
    "DROP TRIGGER IF EXISTS trg_audit_fact_sales ON fact_sales;\n",
    "CREATE TRIGGER trg_audit_fact_sales\n",
    "AFTER INSERT OR UPDATE ON fact_sales\n",
    "FOR EACH ROW EXECUTE FUNCTION log_fact_sales_change();\n",
    "\"\"\"))\n",
    "\n",
    "print(\\\"\\\\n✅ Pipeline complet exécuté avec succès.\\\")\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74072ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e84efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb2a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074bd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focntion de lecture du fichier d'un fichier texte.\n",
    "import os\n",
    "\n",
    "def read_file():\n",
    "    file_path = input(\"Enter the file path (e.g., 'consigne.txt'): \")\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    else:\n",
    "        return \"Le fichier spécifié n'existe pas.\"\n",
    "\n",
    "# Appel de la fonction\n",
    "content = read_file()\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# recuperons les longitude lattitude des stores location\n",
    "%pip install geopy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1357c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5361819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
